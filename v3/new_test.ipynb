{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Dict, Tuple\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import librosa\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    #!/usr/bin/env python\n",
    "    # encoding: utf-8\n",
    "    #\n",
    "    # Copyright 2022 Spotify AB\n",
    "    #\n",
    "    # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "    # you may not use this file except in compliance with the License.\n",
    "    # You may obtain a copy of the License at\n",
    "    #\n",
    "    #     http://www.apache.org/licenses/LICENSE-2.0\n",
    "    #\n",
    "    # Unless required by applicable law or agreed to in writing, software\n",
    "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "    # See the License for the specific language governing permissions and\n",
    "    # limitations under the License.\n",
    "\n",
    "    FFT_HOP = 256\n",
    "    N_FFT = 8 * FFT_HOP\n",
    "\n",
    "    NOTES_BINS_PER_SEMITONE = 1\n",
    "    CONTOURS_BINS_PER_SEMITONE = 3\n",
    "    # base frequency of the CENTRAL bin of the first semitone (i.e., the\n",
    "    # second bin if annotations_bins_per_semitone is 3)\n",
    "    ANNOTATIONS_BASE_FREQUENCY = 27.5  # lowest key on a piano\n",
    "    ANNOTATIONS_N_SEMITONES = 88  # number of piano keys\n",
    "    AUDIO_SAMPLE_RATE = 22050\n",
    "    AUDIO_N_CHANNELS = 1\n",
    "    N_FREQ_BINS_NOTES = ANNOTATIONS_N_SEMITONES * NOTES_BINS_PER_SEMITONE\n",
    "    N_FREQ_BINS_CONTOURS = ANNOTATIONS_N_SEMITONES * CONTOURS_BINS_PER_SEMITONE\n",
    "\n",
    "    AUDIO_WINDOW_LENGTH = 2  # duration in seconds of training examples - original 1\n",
    "\n",
    "    ANNOTATIONS_FPS = AUDIO_SAMPLE_RATE // FFT_HOP\n",
    "    ANNOTATION_HOP = 1.0 / ANNOTATIONS_FPS\n",
    "\n",
    "    # ANNOT_N_TIME_FRAMES is the number of frames in the time-frequency representations we compute\n",
    "    ANNOT_N_FRAMES = ANNOTATIONS_FPS * AUDIO_WINDOW_LENGTH\n",
    "\n",
    "    # AUDIO_N_SAMPLES is the number of samples in the (clipped) audio that we use as input to the models\n",
    "    AUDIO_N_SAMPLES = AUDIO_SAMPLE_RATE * AUDIO_WINDOW_LENGTH - FFT_HOP\n",
    "\n",
    "    DATASET_SAMPLING_FREQUENCY = {\n",
    "        \"MAESTRO\": 5,\n",
    "        \"GuitarSet\": 2,\n",
    "        \"MedleyDB-Pitch\": 2,\n",
    "        \"iKala\": 2,\n",
    "        \"slakh\": 2,\n",
    "    }\n",
    "\n",
    "\n",
    "    def _freq_bins(bins_per_semitone: int, base_frequency: float, n_semitones: int) -> np.array:\n",
    "        d = 2.0 ** (1.0 / (12 * bins_per_semitone))\n",
    "        bin_freqs = base_frequency * d ** np.arange(bins_per_semitone * n_semitones)\n",
    "        return bin_freqs\n",
    "\n",
    "\n",
    "    FREQ_BINS_NOTES = _freq_bins(NOTES_BINS_PER_SEMITONE, ANNOTATIONS_BASE_FREQUENCY, ANNOTATIONS_N_SEMITONES)\n",
    "    FREQ_BINS_CONTOURS = _freq_bins(CONTOURS_BINS_PER_SEMITONE, ANNOTATIONS_BASE_FREQUENCY, ANNOTATIONS_N_SEMITONES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_audio_input(\n",
    "    audio_path: Union[pathlib.Path, str], overlap_len: int, hop_size: int\n",
    ") -> Tuple[jnp.ndarray, List[Dict[str, int]], int]:\n",
    "    \"\"\"\n",
    "    Read wave file (as mono), pad appropriately, and return as\n",
    "    windowed signal, with window length = AUDIO_N_SAMPLES\n",
    "\n",
    "    Returns:\n",
    "        audio_windowed: ndarray with shape (n_windows, AUDIO_N_SAMPLES, 1)\n",
    "            audio windowed into fixed length chunks\n",
    "        window_times: list of {'start':.., 'end':...} objects (times in seconds)\n",
    "        audio_original_length: int\n",
    "            length of original audio file, in frames, BEFORE padding.\n",
    "\n",
    "    \"\"\"\n",
    "    assert overlap_len % 2 == 0, \"overlap_length must be even, got {}\".format(overlap_len)\n",
    "\n",
    "    audio_original, _ = librosa.load(str(audio_path), sr=Constants.AUDIO_SAMPLE_RATE, mono=True)\n",
    "\n",
    "    original_length = audio_original.shape[0]\n",
    "    audio_original = jnp.concatenate([jnp.zeros((int(overlap_len / 2),), dtype=jnp.float32), audio_original])\n",
    "    audio_windowed, window_times = window_audio_file(audio_original, hop_size)\n",
    "    return audio_windowed, window_times, original_length\n",
    "\n",
    "def window_audio_file(audio_original: jnp.ndarray, hop_size: int) -> Tuple[jnp.ndarray, List[Dict[str, int]]]:\n",
    "    \"\"\"\n",
    "    Pad appropriately an audio file, and return as\n",
    "    windowed signal, with window length = AUDIO_N_SAMPLES\n",
    "\n",
    "    Returns:\n",
    "        audio_windowed: ndarray with shape (n_windows, AUDIO_N_SAMPLES, 1)\n",
    "            audio windowed into fixed length chunks\n",
    "        window_times: list of {'start':.., 'end':...} objects (times in seconds)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    audio_windowed = jnp.expand_dims(\n",
    "        frame(audio_original, Constants.AUDIO_N_SAMPLES, hop_size, pad_end=True, pad_value=0),\n",
    "        axis=-1,\n",
    "    )\n",
    "    window_times = [\n",
    "        {\n",
    "            \"start\": t_start,\n",
    "            \"end\": t_start + (Constants.AUDIO_N_SAMPLES / Constants.AUDIO_SAMPLE_RATE),\n",
    "        }\n",
    "        for t_start in jnp.arange(audio_windowed.shape[0]) * hop_size / Constants.AUDIO_SAMPLE_RATE\n",
    "    ]\n",
    "    return audio_windowed, window_times\n",
    "\n",
    "def frame(signal, frame_length, frame_step, pad_end=False, pad_value=0, axis=-1):\n",
    "    \"\"\"\n",
    "    equivalent of tf.signal.frame\n",
    "    \"\"\"\n",
    "    signal_length = signal.shape[axis]\n",
    "    if pad_end:\n",
    "        frames_overlap = frame_length - frame_step\n",
    "        rest_samples = jnp.abs(signal_length - frames_overlap) % jnp.abs(frame_length - frames_overlap)\n",
    "        pad_size = int(frame_length - rest_samples)\n",
    "        if pad_size != 0:\n",
    "            pad_axis = [0] * signal.ndim\n",
    "            pad_axis[axis] = pad_size\n",
    "            signal = jnp.pad(signal, pad_axis, \"constant\", constant_values=pad_value)\n",
    "    frames=signal.unfold(axis, frame_length, frame_step)\n",
    "    return frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrunali/opt/miniconda3/envs/basic-pitch/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.        , 0.        , ..., 0.00096924, 0.00123984,\n",
       "        0.        ], dtype=float32),\n",
       " 11025)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_overlapping_frames = 30\n",
    "overlap_len = n_overlapping_frames * Constants.FFT_HOP\n",
    "hop_size = Constants.AUDIO_N_SAMPLES - overlap_len\n",
    "audio_path = \"test.m4a\"\n",
    "# audio_windowed, _, audio_original_length = get_audio_input(audio_path, overlap_len, hop_size)\n",
    "audio = librosa.load(audio_path, sr=11025)\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrunali/opt/miniconda3/envs/basic-pitch/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 84, 1, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cqt_and_hs import harmonic_stacking, load_and_cqt\n",
    "audio_path = \"test.m4a\"\n",
    "audio_tensor = load_and_cqt(audio_path)\n",
    "def new_f(audio_tensor):\n",
    "    hs = harmonic_stacking(audio_tensor)\n",
    "    return hs\n",
    "model = hk.transform(new_f)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params = model.init(rng, audio_tensor)\n",
    "params\n",
    "out = model.apply(params, rng=rng, audio_tensor=audio_tensor)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformed(init=<function without_state.<locals>.init_fn at 0x15d359160>, apply=<function without_state.<locals>.apply_fn at 0x15d3590d0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/mrunali/Documents/fa22/cs182/bad-pitches/v3',\n",
       " '/Users/mrunali/opt/miniconda3/envs/basic-pitch/lib/python39.zip',\n",
       " '/Users/mrunali/opt/miniconda3/envs/basic-pitch/lib/python3.9',\n",
       " '/Users/mrunali/opt/miniconda3/envs/basic-pitch/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/Users/mrunali/opt/miniconda3/envs/basic-pitch/lib/python3.9/site-packages']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import saved_model\n",
    "\n",
    "\n",
    "model_or_model_path = \"../src/basic_pitch/saved_models/icassp_2022/nmp\"\n",
    "model = saved_model.load(str(model_or_model_path))\n",
    "#get the weights of the model\n",
    "weights = model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hleloeoeoeoo\n",
      "hi\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "f() missing 1 required positional argument: 'model_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m#model_dict = {\"dummy\": \"hi\"}\u001b[39;00m\n\u001b[1;32m     15\u001b[0m params, state \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit(rng, out, \u001b[39mTrue\u001b[39;00m, weights)\n\u001b[0;32m---> 16\u001b[0m model\u001b[39m.\u001b[39;49mapply(params, state, rng\u001b[39m=\u001b[39;49mrng, x\u001b[39m=\u001b[39;49mout, is_training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/basic-pitch/lib/python3.9/site-packages/haiku/_src/transform.py:357\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39mwith\u001b[39;00m base\u001b[39m.\u001b[39mnew_context(params\u001b[39m=\u001b[39mparams, state\u001b[39m=\u001b[39mstate, rng\u001b[39m=\u001b[39mrng) \u001b[39mas\u001b[39;00m ctx:\n\u001b[1;32m    356\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    358\u001b[0m   \u001b[39mexcept\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    359\u001b[0m     \u001b[39mraise\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: f() missing 1 required positional argument: 'model_dict'"
     ]
    }
   ],
   "source": [
    "from new_model_in_jax import PosteriorgramModel\n",
    "import haiku as hk\n",
    "import jax\n",
    "\n",
    "audio_tensor = jnp.array(audio[0])\n",
    "# print(audio_tensor.shape)\n",
    "# audio_tensor = audio_tensor.reshape(2, 2, 14171)\n",
    "def f(x, is_training, model_dict):\n",
    "    a = PosteriorgramModel(model_dict=model_dict)\n",
    "    # a = hk.IdentityCore()\n",
    "    return a(x, is_training)\n",
    "model = hk.transform_with_state(f)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params, state = model.init(rng, out, True, weights)\n",
    "model.apply(params, state, rng=rng, x=out, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('basic-pitch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c94ff12c6dbaf292566dd819f181174b513b49166dc014182483d139491697d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
