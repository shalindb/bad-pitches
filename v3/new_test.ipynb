{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Dict, Tuple\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import librosa\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    #!/usr/bin/env python\n",
    "    # encoding: utf-8\n",
    "    #\n",
    "    # Copyright 2022 Spotify AB\n",
    "    #\n",
    "    # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "    # you may not use this file except in compliance with the License.\n",
    "    # You may obtain a copy of the License at\n",
    "    #\n",
    "    #     http://www.apache.org/licenses/LICENSE-2.0\n",
    "    #\n",
    "    # Unless required by applicable law or agreed to in writing, software\n",
    "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "    # See the License for the specific language governing permissions and\n",
    "    # limitations under the License.\n",
    "\n",
    "    FFT_HOP = 256\n",
    "    N_FFT = 8 * FFT_HOP\n",
    "\n",
    "    NOTES_BINS_PER_SEMITONE = 1\n",
    "    CONTOURS_BINS_PER_SEMITONE = 3\n",
    "    # base frequency of the CENTRAL bin of the first semitone (i.e., the\n",
    "    # second bin if annotations_bins_per_semitone is 3)\n",
    "    ANNOTATIONS_BASE_FREQUENCY = 27.5  # lowest key on a piano\n",
    "    ANNOTATIONS_N_SEMITONES = 88  # number of piano keys\n",
    "    AUDIO_SAMPLE_RATE = 22050\n",
    "    AUDIO_N_CHANNELS = 1\n",
    "    N_FREQ_BINS_NOTES = ANNOTATIONS_N_SEMITONES * NOTES_BINS_PER_SEMITONE\n",
    "    N_FREQ_BINS_CONTOURS = ANNOTATIONS_N_SEMITONES * CONTOURS_BINS_PER_SEMITONE\n",
    "\n",
    "    AUDIO_WINDOW_LENGTH = 2  # duration in seconds of training examples - original 1\n",
    "\n",
    "    ANNOTATIONS_FPS = AUDIO_SAMPLE_RATE // FFT_HOP\n",
    "    ANNOTATION_HOP = 1.0 / ANNOTATIONS_FPS\n",
    "\n",
    "    # ANNOT_N_TIME_FRAMES is the number of frames in the time-frequency representations we compute\n",
    "    ANNOT_N_FRAMES = ANNOTATIONS_FPS * AUDIO_WINDOW_LENGTH\n",
    "\n",
    "    # AUDIO_N_SAMPLES is the number of samples in the (clipped) audio that we use as input to the models\n",
    "    AUDIO_N_SAMPLES = AUDIO_SAMPLE_RATE * AUDIO_WINDOW_LENGTH - FFT_HOP\n",
    "\n",
    "    DATASET_SAMPLING_FREQUENCY = {\n",
    "        \"MAESTRO\": 5,\n",
    "        \"GuitarSet\": 2,\n",
    "        \"MedleyDB-Pitch\": 2,\n",
    "        \"iKala\": 2,\n",
    "        \"slakh\": 2,\n",
    "    }\n",
    "\n",
    "\n",
    "    def _freq_bins(bins_per_semitone: int, base_frequency: float, n_semitones: int) -> np.array:\n",
    "        d = 2.0 ** (1.0 / (12 * bins_per_semitone))\n",
    "        bin_freqs = base_frequency * d ** np.arange(bins_per_semitone * n_semitones)\n",
    "        return bin_freqs\n",
    "\n",
    "\n",
    "    FREQ_BINS_NOTES = _freq_bins(NOTES_BINS_PER_SEMITONE, ANNOTATIONS_BASE_FREQUENCY, ANNOTATIONS_N_SEMITONES)\n",
    "    FREQ_BINS_CONTOURS = _freq_bins(CONTOURS_BINS_PER_SEMITONE, ANNOTATIONS_BASE_FREQUENCY, ANNOTATIONS_N_SEMITONES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_audio_input(\n",
    "    audio_path: Union[pathlib.Path, str], overlap_len: int, hop_size: int\n",
    ") -> Tuple[jnp.ndarray, List[Dict[str, int]], int]:\n",
    "    \"\"\"\n",
    "    Read wave file (as mono), pad appropriately, and return as\n",
    "    windowed signal, with window length = AUDIO_N_SAMPLES\n",
    "\n",
    "    Returns:\n",
    "        audio_windowed: ndarray with shape (n_windows, AUDIO_N_SAMPLES, 1)\n",
    "            audio windowed into fixed length chunks\n",
    "        window_times: list of {'start':.., 'end':...} objects (times in seconds)\n",
    "        audio_original_length: int\n",
    "            length of original audio file, in frames, BEFORE padding.\n",
    "\n",
    "    \"\"\"\n",
    "    assert overlap_len % 2 == 0, \"overlap_length must be even, got {}\".format(overlap_len)\n",
    "\n",
    "    audio_original, _ = librosa.load(str(audio_path), sr=Constants.AUDIO_SAMPLE_RATE, mono=True)\n",
    "\n",
    "    original_length = audio_original.shape[0]\n",
    "    audio_original = jnp.concatenate([jnp.zeros((int(overlap_len / 2),), dtype=jnp.float32), audio_original])\n",
    "    audio_windowed, window_times = window_audio_file(audio_original, hop_size)\n",
    "    return audio_windowed, window_times, original_length\n",
    "\n",
    "def window_audio_file(audio_original: jnp.ndarray, hop_size: int) -> Tuple[jnp.ndarray, List[Dict[str, int]]]:\n",
    "    \"\"\"\n",
    "    Pad appropriately an audio file, and return as\n",
    "    windowed signal, with window length = AUDIO_N_SAMPLES\n",
    "\n",
    "    Returns:\n",
    "        audio_windowed: ndarray with shape (n_windows, AUDIO_N_SAMPLES, 1)\n",
    "            audio windowed into fixed length chunks\n",
    "        window_times: list of {'start':.., 'end':...} objects (times in seconds)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    audio_windowed = jnp.expand_dims(\n",
    "        frame(audio_original, Constants.AUDIO_N_SAMPLES, hop_size, pad_end=True, pad_value=0),\n",
    "        axis=-1,\n",
    "    )\n",
    "    window_times = [\n",
    "        {\n",
    "            \"start\": t_start,\n",
    "            \"end\": t_start + (Constants.AUDIO_N_SAMPLES / Constants.AUDIO_SAMPLE_RATE),\n",
    "        }\n",
    "        for t_start in jnp.arange(audio_windowed.shape[0]) * hop_size / Constants.AUDIO_SAMPLE_RATE\n",
    "    ]\n",
    "    return audio_windowed, window_times\n",
    "\n",
    "def frame(signal, frame_length, frame_step, pad_end=False, pad_value=0, axis=-1):\n",
    "    \"\"\"\n",
    "    equivalent of tf.signal.frame\n",
    "    \"\"\"\n",
    "    signal_length = signal.shape[axis]\n",
    "    if pad_end:\n",
    "        frames_overlap = frame_length - frame_step\n",
    "        rest_samples = jnp.abs(signal_length - frames_overlap) % jnp.abs(frame_length - frames_overlap)\n",
    "        pad_size = int(frame_length - rest_samples)\n",
    "        if pad_size != 0:\n",
    "            pad_axis = [0] * signal.ndim\n",
    "            pad_axis[axis] = pad_size\n",
    "            signal = jnp.pad(signal, pad_axis, \"constant\", constant_values=pad_value)\n",
    "    frames=signal.unfold(axis, frame_length, frame_step)\n",
    "    return frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.        , 0.        , ..., 0.00098911, 0.00120955,\n",
       "        0.        ], dtype=float32),\n",
       " 11025)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_overlapping_frames = 30\n",
    "overlap_len = n_overlapping_frames * Constants.FFT_HOP\n",
    "hop_size = Constants.AUDIO_N_SAMPLES - overlap_len\n",
    "audio_path = \"test.m4a\"\n",
    "# audio_windowed, _, audio_original_length = get_audio_input(audio_path, overlap_len, hop_size)\n",
    "audio = librosa.load(audio_path, sr=11025)\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56684,)\n",
      "[[[0.49934176]\n",
      "  [0.50073373]]\n",
      "\n",
      " [[0.5010107 ]\n",
      "  [0.50005484]]] [[[0.49957204]]\n",
      "\n",
      " [[0.49912938]]] [[[0.50954604]]\n",
      "\n",
      " [[0.51272285]]]\n",
      "[[[0.49934176]\n",
      "  [0.50073373]]\n",
      "\n",
      " [[0.5010107 ]\n",
      "  [0.50005484]]] [[[0.49957204]]\n",
      "\n",
      " [[0.49912938]]] [[[0.50954604]]\n",
      "\n",
      " [[0.51272285]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[[0.49934176],\n",
       "               [0.50073373]],\n",
       " \n",
       "              [[0.5010107 ],\n",
       "               [0.50005484]]], dtype=float32),\n",
       " DeviceArray([[[0.49957204]],\n",
       " \n",
       "              [[0.49912938]]], dtype=float32),\n",
       " DeviceArray([[[0.50954604]],\n",
       " \n",
       "              [[0.51272285]]], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from new_model_in_jax import PosteriorgramModel\n",
    "import haiku as hk\n",
    "import jax\n",
    "\n",
    "\n",
    "audio_tensor = jnp.array(audio[0])\n",
    "print(audio_tensor.shape)\n",
    "audio_tensor = audio_tensor.reshape(2, 2, 14171)\n",
    "def f(audio_tensor):\n",
    "    a = PosteriorgramModel()\n",
    "    # a = hk.IdentityCore()\n",
    "    return a(audio_tensor)\n",
    "model = hk.transform(f)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params = model.init(rng, audio_tensor)\n",
    "model.apply(params, rng=rng, audio_tensor=audio_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56684,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erankohen/Desktop/cs182/bad-pitches/v3/cqt_harmonic_stacking.py:185: UserWarning: \n",
      "input size = (1, 1, 56684)\tkernel size = 256\n",
      "padding with reflection mode might not be the best choice, try using constant padding\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l8/mrlzwq6j2dlcgdh4m9c07mhr0000gn/T/ipykernel_90797/1347257644.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# model.apply(params, rng=rng, audio_tensor=audio_tensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/haiku/_src/transform.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m       raise ValueError(\"If your transformed function uses `hk.{get,set}_state` \"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/haiku/_src/transform.py\u001b[0m in \u001b[0;36minit_fn\u001b[0;34m(rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedTracerError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedTracerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munexpected_tracer_hint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/l8/mrlzwq6j2dlcgdh4m9c07mhr0000gn/T/ipykernel_90797/1347257644.py\u001b[0m in \u001b[0;36mnew_f\u001b[0;34m(audio_tensor)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mConstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_FREQ_BINS_CONTOURS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/haiku/_src/module.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0;31m# Module names are set in the constructor. If `f` is the constructor then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/haiku/_src/module.py\u001b[0m in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[0;34m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minterceptor_stack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m   ctx = MethodContext(module=self,\n",
      "\u001b[0;32m~/Desktop/cs182/bad-pitches/v3/cqt_harmonic_stacking.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Getting the top octave CQT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mCQT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cqt_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcqt_kernels_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcqt_kernels_imag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mx_down\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m  \u001b[0;31m# Preparing a new variable for downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/cs182/bad-pitches/v3/cqt_harmonic_stacking.py\u001b[0m in \u001b[0;36mget_cqt_complex\u001b[0;34m(x, cqt_kernels_real, cqt_kernels_imag, hop_length, padding)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcqt_kernels_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcqt_kernels_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     CQT_real = jnp.transpose(\n\u001b[0;32m--> 192\u001b[0;31m         lax.conv(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcqt_kernels_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/jax/_src/lax/convolution.py\u001b[0m in \u001b[0;36mconv\u001b[0;34m(lhs, rhs, window_strides, padding, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mAn\u001b[0m \u001b[0marray\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconvolution\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m   return conv_general_dilated(lhs, rhs, window_strides, padding,\n\u001b[0m\u001b[1;32m    193\u001b[0m                               \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                               preferred_element_type=preferred_element_type)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/jax/_src/lax/convolution.py\u001b[0m in \u001b[0;36mconv_general_dilated\u001b[0;34m(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    155\u001b[0m       dtypes.canonicalize_dtype(np.dtype(preferred_element_type)))\n\u001b[1;32m    156\u001b[0m   return conv_general_dilated_p.bind(\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_strides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_strides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m       \u001b[0mlhs_dilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs_dilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_dilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs_dilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0mdimension_numbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdnums\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "from cqt_harmonic_stacking import CQTLayer, HarmonicStackingLayer\n",
    "audio = librosa.load(audio_path, sr=11025)\n",
    "audio_tensor = jnp.array(audio[0])\n",
    "print(audio_tensor.shape)\n",
    "# audio_tensor = audio_tensor.reshape(2, 2, 14171)\n",
    "def new_f(audio_tensor):\n",
    "    a = CQTLayer(audio_tensor.shape)\n",
    "    b = HarmonicStackingLayer(\n",
    "        Constants.CONTOURS_BINS_PER_SEMITONE,\n",
    "        [0.5] + list(range(1, 8)),\n",
    "        Constants.N_FREQ_BINS_CONTOURS,\n",
    "    )\n",
    "    return b(a(audio_tensor))\n",
    "model = hk.transform(new_f)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params = model.init(rng, audio_tensor)\n",
    "params\n",
    "# model.apply(params, rng=rng, audio_tensor=audio_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
