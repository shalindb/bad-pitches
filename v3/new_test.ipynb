{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from typing import Union, List, Dict, Tuple\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import librosa\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    #!/usr/bin/env python\n",
    "    # encoding: utf-8\n",
    "    #\n",
    "    # Copyright 2022 Spotify AB\n",
    "    #\n",
    "    # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "    # you may not use this file except in compliance with the License.\n",
    "    # You may obtain a copy of the License at\n",
    "    #\n",
    "    #     http://www.apache.org/licenses/LICENSE-2.0\n",
    "    #\n",
    "    # Unless required by applicable law or agreed to in writing, software\n",
    "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "    # See the License for the specific language governing permissions and\n",
    "    # limitations under the License.\n",
    "\n",
    "    FFT_HOP = 256\n",
    "    N_FFT = 8 * FFT_HOP\n",
    "\n",
    "    NOTES_BINS_PER_SEMITONE = 1\n",
    "    CONTOURS_BINS_PER_SEMITONE = 3\n",
    "    # base frequency of the CENTRAL bin of the first semitone (i.e., the\n",
    "    # second bin if annotations_bins_per_semitone is 3)\n",
    "    ANNOTATIONS_BASE_FREQUENCY = 27.5  # lowest key on a piano\n",
    "    ANNOTATIONS_N_SEMITONES = 88  # number of piano keys\n",
    "    AUDIO_SAMPLE_RATE = 22050\n",
    "    AUDIO_N_CHANNELS = 1\n",
    "    N_FREQ_BINS_NOTES = ANNOTATIONS_N_SEMITONES * NOTES_BINS_PER_SEMITONE\n",
    "    N_FREQ_BINS_CONTOURS = ANNOTATIONS_N_SEMITONES * CONTOURS_BINS_PER_SEMITONE\n",
    "\n",
    "    AUDIO_WINDOW_LENGTH = 2  # duration in seconds of training examples - original 1\n",
    "\n",
    "    ANNOTATIONS_FPS = AUDIO_SAMPLE_RATE // FFT_HOP\n",
    "    ANNOTATION_HOP = 1.0 / ANNOTATIONS_FPS\n",
    "\n",
    "    # ANNOT_N_TIME_FRAMES is the number of frames in the time-frequency representations we compute\n",
    "    ANNOT_N_FRAMES = ANNOTATIONS_FPS * AUDIO_WINDOW_LENGTH\n",
    "\n",
    "    # AUDIO_N_SAMPLES is the number of samples in the (clipped) audio that we use as input to the models\n",
    "    AUDIO_N_SAMPLES = AUDIO_SAMPLE_RATE * AUDIO_WINDOW_LENGTH - FFT_HOP\n",
    "\n",
    "    DATASET_SAMPLING_FREQUENCY = {\n",
    "        \"MAESTRO\": 5,\n",
    "        \"GuitarSet\": 2,\n",
    "        \"MedleyDB-Pitch\": 2,\n",
    "        \"iKala\": 2,\n",
    "        \"slakh\": 2,\n",
    "    }\n",
    "\n",
    "\n",
    "    def _freq_bins(bins_per_semitone: int, base_frequency: float, n_semitones: int) -> np.array:\n",
    "        d = 2.0 ** (1.0 / (12 * bins_per_semitone))\n",
    "        bin_freqs = base_frequency * d ** np.arange(bins_per_semitone * n_semitones)\n",
    "        return bin_freqs\n",
    "\n",
    "\n",
    "    FREQ_BINS_NOTES = _freq_bins(NOTES_BINS_PER_SEMITONE, ANNOTATIONS_BASE_FREQUENCY, ANNOTATIONS_N_SEMITONES)\n",
    "    FREQ_BINS_CONTOURS = _freq_bins(CONTOURS_BINS_PER_SEMITONE, ANNOTATIONS_BASE_FREQUENCY, ANNOTATIONS_N_SEMITONES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_audio_input(\n",
    "    audio_path: Union[pathlib.Path, str], overlap_len: int, hop_size: int\n",
    ") -> Tuple[jnp.ndarray, List[Dict[str, int]], int]:\n",
    "    \"\"\"\n",
    "    Read wave file (as mono), pad appropriately, and return as\n",
    "    windowed signal, with window length = AUDIO_N_SAMPLES\n",
    "\n",
    "    Returns:\n",
    "        audio_windowed: ndarray with shape (n_windows, AUDIO_N_SAMPLES, 1)\n",
    "            audio windowed into fixed length chunks\n",
    "        window_times: list of {'start':.., 'end':...} objects (times in seconds)\n",
    "        audio_original_length: int\n",
    "            length of original audio file, in frames, BEFORE padding.\n",
    "\n",
    "    \"\"\"\n",
    "    assert overlap_len % 2 == 0, \"overlap_length must be even, got {}\".format(overlap_len)\n",
    "\n",
    "    audio_original, _ = librosa.load(str(audio_path), sr=Constants.AUDIO_SAMPLE_RATE, mono=True)\n",
    "\n",
    "    original_length = audio_original.shape[0]\n",
    "    audio_original = jnp.concatenate([jnp.zeros((int(overlap_len / 2),), dtype=jnp.float32), audio_original])\n",
    "    audio_windowed, window_times = window_audio_file(audio_original, hop_size)\n",
    "    return audio_windowed, window_times, original_length\n",
    "\n",
    "def window_audio_file(audio_original: jnp.ndarray, hop_size: int) -> Tuple[jnp.ndarray, List[Dict[str, int]]]:\n",
    "    \"\"\"\n",
    "    Pad appropriately an audio file, and return as\n",
    "    windowed signal, with window length = AUDIO_N_SAMPLES\n",
    "\n",
    "    Returns:\n",
    "        audio_windowed: ndarray with shape (n_windows, AUDIO_N_SAMPLES, 1)\n",
    "            audio windowed into fixed length chunks\n",
    "        window_times: list of {'start':.., 'end':...} objects (times in seconds)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    audio_windowed = jnp.expand_dims(\n",
    "        frame(audio_original, Constants.AUDIO_N_SAMPLES, hop_size, pad_end=True, pad_value=0),\n",
    "        axis=-1,\n",
    "    )\n",
    "    window_times = [\n",
    "        {\n",
    "            \"start\": t_start,\n",
    "            \"end\": t_start + (Constants.AUDIO_N_SAMPLES / Constants.AUDIO_SAMPLE_RATE),\n",
    "        }\n",
    "        for t_start in jnp.arange(audio_windowed.shape[0]) * hop_size / Constants.AUDIO_SAMPLE_RATE\n",
    "    ]\n",
    "    return audio_windowed, window_times\n",
    "\n",
    "def frame(signal, frame_length, frame_step, pad_end=False, pad_value=0, axis=-1):\n",
    "    \"\"\"\n",
    "    equivalent of tf.signal.frame\n",
    "    \"\"\"\n",
    "    signal_length = signal.shape[axis]\n",
    "    if pad_end:\n",
    "        frames_overlap = frame_length - frame_step\n",
    "        rest_samples = jnp.abs(signal_length - frames_overlap) % jnp.abs(frame_length - frames_overlap)\n",
    "        pad_size = int(frame_length - rest_samples)\n",
    "        if pad_size != 0:\n",
    "            pad_axis = [0] * signal.ndim\n",
    "            pad_axis[axis] = pad_size\n",
    "            signal = jnp.pad(signal, pad_axis, \"constant\", constant_values=pad_value)\n",
    "    frames=signal.unfold(axis, frame_length, frame_step)\n",
    "    return frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noahadhikari/.local/lib/python3.8/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.        , 0.        , ..., 0.00098911, 0.00120955,\n",
       "        0.        ], dtype=float32),\n",
       " 11025)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_overlapping_frames = 30\n",
    "overlap_len = n_overlapping_frames * Constants.FFT_HOP\n",
    "hop_size = Constants.AUDIO_N_SAMPLES - overlap_len\n",
    "audio_path = \"test.m4a\"\n",
    "# audio_windowed, _, audio_original_length = get_audio_input(audio_path, overlap_len, hop_size)\n",
    "audio = librosa.load(audio_path, sr=11025)\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.49934176]\n",
      "  [0.50073373]]\n",
      "\n",
      " [[0.5010107 ]\n",
      "  [0.50005484]]] [[[0.49957204]]\n",
      "\n",
      " [[0.49912938]]] [[[0.50954604]]\n",
      "\n",
      " [[0.51272285]]]\n",
      "[[[0.49934176]\n",
      "  [0.50073373]]\n",
      "\n",
      " [[0.5010107 ]\n",
      "  [0.50005484]]] [[[0.49957204]]\n",
      "\n",
      " [[0.49912938]]] [[[0.50954604]]\n",
      "\n",
      " [[0.51272285]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[[0.49934176],\n",
       "               [0.50073373]],\n",
       " \n",
       "              [[0.5010107 ],\n",
       "               [0.50005484]]], dtype=float32),\n",
       " DeviceArray([[[0.49957204]],\n",
       " \n",
       "              [[0.49912938]]], dtype=float32),\n",
       " DeviceArray([[[0.50954604]],\n",
       " \n",
       "              [[0.51272285]]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from new_model_in_jax import PosteriorgramModel\n",
    "import haiku as hk\n",
    "import jax\n",
    "\n",
    "\n",
    "audio_tensor = jnp.array(audio[0])\n",
    "# print(audio_tensor.shape)\n",
    "audio_tensor = audio_tensor.reshape(2, 2, 14171)\n",
    "def f(audio_tensor):\n",
    "    a = PosteriorgramModel()\n",
    "    # a = hk.IdentityCore()\n",
    "    return a(audio_tensor)\n",
    "model = hk.transform(f)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params = model.init(rng, audio_tensor)\n",
    "model.apply(params, rng=rng, audio_tensor=audio_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
